{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc97c392-66f2-4c4e-9942-421132fc39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Dict\n",
    "from config import Settings\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "from typing import Annotated, List, Tuple, Union\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4c8a5-1477-40d9-ba5a-d66e18f685be",
   "metadata": {},
   "source": [
    "### Define OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325e1fbf-ed04-4637-af0e-8cc7e857cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = Settings.api_key\n",
    "VECTOR_DIR = 'vectorize'\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbfb4b1-2756-4eb3-8751-c60a95825df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "    Answer the user's questions based on the below context. \n",
    "    If the context doesn't contain any relevant information to the questions, don't make someting up\n",
    "    and just reply information cannot be fount:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846ff77-97c9-4580-84d5-c5a7c2229338",
   "metadata": {},
   "source": [
    "### Define Tools\n",
    "\n",
    "We first define the vectorstore where we will be storing our memories. Memories will be stored as embeddings and later looked up based on the converstation context. We are using an in-memory vectorstore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752de05d-b694-4f9a-8437-d0321938481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "def get_user_id(config: RunnableConfig) -> str:\n",
    "\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    if user_id in None:\n",
    "        raise ValueError(\"User ID needs to be provided to save memory\")\n",
    "    return user_id\n",
    "\n",
    "@tool\n",
    "def save_recall_memory(memory: str, config: RunnableConfig) -> str:\n",
    "\n",
    "    \"Save memory to vectorstore for later semantic search retrieval.\"\n",
    "    user_id = get_user_id(config)\n",
    "    document = Document(page_content=memory, id=str(uuid.uuid4()), metadata={\"user_id\": user_id})\n",
    "    recall_vector_store.add_document([document])\n",
    "\n",
    "@tool\n",
    "def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:\n",
    "    \"\"\"Search for relevant memories.\"\"\"\n",
    "\n",
    "    user_id = get_user_id(config)\n",
    "\n",
    "    def _filter_function(doc: Document) -> bool:\n",
    "        return doc.metadata.get(\"user_id\") == user_id\n",
    "\n",
    "    documents = recall_vector_store.similarity_search(query, k=3, filter=_filter_function)\n",
    "    return [document.page_content for document in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8744fe4-34ab-440e-94ef-32dc9bfc6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "@tool\n",
    "def contextualQA(earnings_question: str, company_name: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Identifies the relevant context in the earning call transcripts to the user question\n",
    "\n",
    "    This tool searches in the earnings calls transcript documents and extract financial information\n",
    "    such as net income, REvenue, EBITDA and etc.\n",
    "\n",
    "    Parameters:\n",
    "    - earnings_question: The questions asked by the user\n",
    "    - the company for which we need to answer the question. Company names should always be lowercase\n",
    "\n",
    "    Returns:\n",
    "    - A string with the context that contain the answer to the earnings question.\n",
    "    \"\"\"\n",
    "    vector_store = FAISS.load_local(VECTOR_DIR + \"/\" + company_name, \n",
    "                                    embeddings, \n",
    "                                    allow_dangerous_deserialization=True)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", SYSTEM_TEMPLATE),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    retrieval_chain = RunnablePassthrough.assign(\n",
    "    context = parse_retriever_input | retriever).assign(answer=qa_chain)\n",
    "    response = retrieval_chain.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=earnings_question)]\n",
    "        }\n",
    "    )\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f69253-314b-42da-a553-585f1b52cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [save_recall_memory, search_recall_memories, contextualQA]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab638068-97b7-47f7-a1b3-7dff59c374e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VectorStore for memories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
